Welcome back.
So I thought it would make for better understanding
if I showed you the each to auto Emma workflow
before we implemented it.
And along the way, you're also going to learn some important
ideas and terms that are so associated with issue.
Oromo. So for a few moments, I'm going to switch back
to the slide deck by Aaron, Who's the chief machine learning
scientist at, actually, I All right, Um oh, right before I go
into the explanations, let me tell you how you can find
this slide deck.
So you just need to click on this hamburger icon
and from the bookmarks tab in the drop down, you'll see, um,
the H 20 slide deck present.
So you just click on that and you'll be directed here.
All right, So this is what the general workflow for each
to autumn L and Python looks like we first import Itcho
and Estelle Auto ml.
And then we start up something called a cluster, using the
assured out innit command.
And then we can import our data, run the auto ml
functionality and train our model and finally observe the
best performing models in the leader board.
So if you're wondering what an H 20 cluster is, I just
briefly mentioned it now and in the first task.
So it's Chuo is actually a distributed machine learning
platform or a distributed machine learning computing system.
So this is just nomenclature.
So we have something that we call an H two a cluster,
and the first thing you do in our in any python issue
a script would be starting up the a stroll cluster.
And what that really means is that you're just starting up
a process where all the computations are going to happen
and all the data is going to live in there.
It's just a basically a block of memory where all the models
and the data live and all the computations happen.
So that's what we mean when we're referring to an actual
cluster and there's no limit on the size of the cluster.
It could be on your single laptop, or it could be across a
very large network of physical machines or even a digital
cluster.
But everything that we are going to be working on today
will be on a local single computer, and most the time
you don't even know.
Need to know these things is just good toe have, and so that
you don't have to switch over to some other different system
or framework when you're ready to scale or use a very large
data sets.
And then there's something called an itch dual frame,
and this is basically just a data frame.
So if you're in Python, you have pandas data frame, basically
the same thing just underneath the rose.
If you're using multiple notes, some of the roads would be
on one note, and some of the rose would be on another one.
So underneath it's distributive.
But when you're dealing with it and are or by found
like we are, you don't need to know that.
And, um, all of this detail is abstracted away from you.
So they've kept this in tax for a stroll, frames very much
like pandas data frames, the difference being that just not
all of the functionality is implemented.
So, like what we're doing, we can always do our data munching
wrangling, feature engineering with pandas, data frames.
And once we're done with all the slicing, combining what have
you in pandas?
We can take that data frame and just turn it into an actual
frame that can be used, Um, in auto ml.
Okay, so let's go back to our notebook and start up the usual
cluster.
And we do this under the task force subheading.
So just like in the slide we looked at a few moments ago,
we need to import H 20 and initialize the H 20 cluster h
20 dot in it and thes brackets here.
And once you're done, feel free to hit shift Enter.
It should take about a few seconds to print everything
to screen.
So here we see that, um, this cluster is running at local
host at this port 54321 And it tells us how long the cluster
has been up for, um, and the actual version as well.
So since we just started the cluster, you conceits
for 22 seconds.
But if you already had a cluster running previously
and you use thean, it command, it would show you the total up
time. We can also see other useful information
like the actual ah version and also the default memory
allocated to the A stroll cluster.
So This is something that you're not probably not used
to thinking about setting aside memory.
So, um, if we want to increase it if you have enough ram
on your machine, you can always pass an argument when
you're starting the ah in it method or cluster called Max
meme size.
So it's just Max underscore meme underscore size, and you
can set it to something like, UH 10 g for 10 gigabytes
if you have this much of a ram on your machine.
So a rule of thumb, I would suggest, is you need
between three and four times the amount of memory Engram.
Ah, then the size of your data on disk.
However, it's really not something to think
about unless something doesn't work, and then you can always
come back and increase the allocated memory.
Okay, so now that we have started up our ritual cluster, we
can convert our pandas data frame DF into an actual frame,
and it's extremely simple to do that.
So let's call our H 20 frame just so that it's not ambiguous
h to underscore Dia and go ahead and use this function called
H 20 frame, where h O. F and R are capitalized.
Sorry are is lower case.
Forgive me, and all we need to do is pass in our Pandas data
frame just like that.
There we go.
Let's go ahead and create this issue of frame.
It should again take a few seconds and you'll see the parts
progress bar right below.
And when it's that 100% we can go ahead and take a look
at the data.
So what?
This function that we just ran does actual frame is
because this data is actually sitting in python memory
the only way to get it into the actual cluster, which
under the hood uses java.
So the only way to get it into a Java memory is actually
to write it disk and read it back into the HDL cluster, using
the same file functionality.
All right, now that it's on each door frame, let's take
a look at what it looks like so we can use the describe
method the same as you would type in in panda's and press
shift enter so you can see that we have about 45,000 rows
and 17 columns, and you see the same variables here and ah,
in the in the row called type, you'll see the different data
types. So there's integers, and there's something that
if you're not used to Java, you might be confused by.
So there's this data type called in, um and ah, since H 20 is
actually Java underneath the name for categorical data
in Java is in, Um, and if you're usedto are you would be used
to calling it a factor.
And in pandas, it goes by the name categorical data.
So here, ah, let's scroll to the right of it.
You'll notice that our response call him, which is term
underscore.
Deposit is also an in, um, it's already encoded
as categorical data type, so there's nothing we need to do
here. But if it were encoded as an integer say zero and one
for binary values, then we'd have to convert the column
into a factor as follows.
Where why here is the term deposit column, and we just used
the as factor method.
And when you run that it would convert the term deposit, uh,
call him into a genome data type, all right,
but we don't need to do that now.
since it was recognized, as in, um by default.
Okay, so now we are going to need to define our X and y's
and the training frame.
So why is either the name or the index of the response
variable?
So that's just term deposit.
So let's do that here.
Eso let's do this below.
So here we are identifying are predictors and response.
So in a moment we're going to create a train data frame,
and so X is just going to be the names of all the columns.
And why is R response variable, which is term underscore
deposit?
Great. And now let's go ahead and make sure we create our
train intestines, and if you're used to psych it, learn you
might be used to, ah, typing in the train test split method.
But here we can just use the data frame H 20 frame.
Forgive me and use a split underscore frame method to achieve
the same goals.
So we call it stool underscore D f dot split frame, and here
we can specify the ratios of train and test.
So let's split it.
Let's say 75 25% so 75% of the data is going to be allocated
to the tests.
The training set There we go, so ratios equals 0.75 And once
we have created our training test sets, we can identify
the predictors and response.
And we also need to make sure that we remove the term deposit
response from the list of predictors so we can just use the
remove method, since this is a list and remove the term
deposit response variable.
So once you're done with this, feel free to go ahead and hit
shift and enter to create your train intestines and also
identify the predictors and response.
Awesome.
We now have our data ready so we can finally run Auto Ml
using a shoe.
Oh, and let's do that in the next task to run various models
using auto ml and find and let auto mail find the best one
for us.